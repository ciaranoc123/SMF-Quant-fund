{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is used to populate the DB with stock data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-I- Successful database connection\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Import all the necessary packages and smf modules and check if database connection works.\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "from smf.db_engine import DbEngine\n",
    "import eikon as ek\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "\n",
    "# try:\n",
    "#     with open(f\"smf\\eikon_app_key.json\") as ek_app_key:\n",
    "#             ek_app_key = json.load(ek_app_key)\n",
    "#     ek.set_app_key(ek_app_key[\"key\"])\n",
    "# except:\n",
    "#     ek.set_app_key(\"d14a6a14480842328de35bc001771c605538616a\")    \n",
    "ek.set_app_key('403255e90c7647afafbfb5c0000d60ac4c8cc536')\n",
    "db = DbEngine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-W- Table ticker_symbols not found\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_29844/1270271495.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0msectors_table_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"tickers_and_classifications\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdb_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch_table\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ticker_symbols\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mtickers_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdb_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"ric\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0meikon_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mek\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtickers_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"TR.TRBCEconomicSector\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"TR.TRBCBusinessSector\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"TR.TRBCIndustryGroup\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "\"\"\" Create table \"tickers_and_classifications\" which contains details on the economic sector, business sector\n",
    "    and industry group for each given RIC.\n",
    "\"\"\"\n",
    "sectors_table_name = \"tickers_and_classifications\"\n",
    "db_data = db.fetch_table(\"ticker_symbols\")\n",
    "tickers_list = db_data[\"ric\"].values.tolist()\n",
    "eikon_data = ek.get_data(tickers_list, [\"TR.TRBCEconomicSector\",\"TR.TRBCBusinessSector\", \"TR.TRBCIndustryGroup\"])\n",
    "\n",
    "sectors_df = pd.DataFrame(data=eikon_data[0])\n",
    "sectors_df.columns = [\"ric\", \"economic_sector\", \"business_sector\", \"industry_group\"]\n",
    "sectors_df = sectors_df.set_index(\"ric\")\n",
    "\n",
    "db.append_df_as_row(sectors_df, sectors_table_name, if_exists=\"replace\", index=True)\n",
    "db.set_primary_key(sectors_table_name, \"ric\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-W- Table ticker_symbols not found\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_29844/1307344884.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mrelative\u001b[0m \u001b[0mprice\u001b[0m \u001b[0mchange\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mn\u001b[0m \u001b[0mmost\u001b[0m \u001b[0mrecent\u001b[0m \u001b[0mtrading\u001b[0m \u001b[0mdays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpressed\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0ma\u001b[0m \u001b[0mpercentage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \"\"\"\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtickers_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch_table\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ticker_symbols\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"ric\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mvolatilities_days\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m25\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m40\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m60\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m80\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m90\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m120\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m150\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m160\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m180\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m240\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m250\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m260\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mvolatilities_fields\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mnum\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34mf\"TR.Volatility{str(num)}D\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvolatilities_days\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "\"\"\" Create table \"volatilies_as_at_{date}\" which contains the annualized standard deviation\n",
    "    of the relative price change for the n most recent trading days, expressed as a percentage.\n",
    "\"\"\"\n",
    "tickers_list = db.fetch_table(\"ticker_symbols\")[\"ric\"].values.tolist()\n",
    "volatilities_days = [2, 5, 10, 20, 25, 30, 40, 50, 60, 80, 90, 100, 120, 150, 160, 180, 200, 240, 250, 260]\n",
    "volatilities_fields = list(map(lambda num:f\"TR.Volatility{str(num)}D\", volatilities_days))\n",
    "eikon_data, err = ek.get_data(tickers_list, volatilities_fields)\n",
    "volatilities_df = pd.DataFrame(eikon_data)\n",
    "volatilities_df.columns = [\"ric\"] + list(map(lambda num:f\"{str(num)}_day_volatility\", volatilities_days))\n",
    "volatilities_df = volatilities_df.set_index(\"ric\")\n",
    "\n",
    "today = dt.datetime.now().strftime(\"%Y_%m_%d\")\n",
    "volatilities_table_name = f\"volatilities_as_at_{today}\"\n",
    "db.append_df_as_row(volatilities_df, volatilities_table_name, if_exists=\"replace\", index=True)\n",
    "db.set_primary_key(volatilities_table_name, \"ric\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\" This is the most problematic part...\n",
    "    For each stock/ticker/RIC, create a table containing the following information for that stock:\n",
    "        - Closing prices\n",
    "        - PE ratio\n",
    "        - PB ratio\n",
    "        - PFCF ratio\n",
    "        - EV-EBITDA ratio\n",
    "\"\"\" \n",
    "\n",
    "time_ = time.time()\n",
    "tickers_list= db.fetch_table(\"ticker_symbols\")[\"ric\"].values.tolist()\n",
    "start = dt.date(2016, 6, 30).strftime(\"%Y-%m-%d\")\n",
    "end = dt.datetime.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "\n",
    "problem_stocks = []\n",
    "\n",
    "for (count, ticker) in reversed(list(enumerate(tickers_list))):\n",
    "    try:\n",
    "        # Closing prices\n",
    "        ek_closing_prices = ek.get_timeseries(ticker, [\"Close\"], start_date=start, end_date=end)\n",
    "        ticker_info_df = pd.DataFrame(ek_closing_prices)\n",
    "        ticker_info_df.columns = [\"close_price\"]\n",
    "\n",
    "        # Price-earnings\n",
    "        date_field = f\"TR.HistPE(SDate={start},EDate={end},Period=FI0,Frq=FI).date\"\n",
    "        field = f\"TR.HistPE(SDate={start},EDate={end},Period=FI0,Frq=FI)\"\n",
    "        eik_data, err = ek.get_data(ticker, [date_field, field])\n",
    "        data_df = pd.DataFrame(eik_data, columns=[\"Date\", \"Historic P/E\"]).set_index(\"Date\")\n",
    "        if data_df[\"Historic P/E\"].isna().all():\n",
    "            ticker_info_df[\"price_earnings\"] = np.nan\n",
    "        else:\n",
    "            data_df.columns = [\"price_earnings\"]\n",
    "            data_df.index = data_df.index.str[0:10]#pd.to_datetime(data_df.index, format=\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "            ticker_info_df = ticker_info_df.join(data_df)\n",
    "            ticker_info_df = ticker_info_df.fillna(method=\"ffill\")\n",
    "\n",
    "        # Price-to-book\n",
    "        date_field = f\"TR.PriceToBVPerShare(SDate={start},EDate={end},Frq=D).date\"\n",
    "        field = f\"TR.PriceToBVPerShare(SDate={start},EDate={end},Frq=D)\"\n",
    "        eik_data, err = ek.get_data(ticker, [date_field, field])\n",
    "        data_df = pd.DataFrame(eik_data, columns=[\"Date\", \"Price To Book Value Per Share (Daily Time Series Ratio)\"]).set_index(\"Date\")\n",
    "        if data_df[\"Price To Book Value Per Share (Daily Time Series Ratio)\"].isna().all():\n",
    "            ticker_info_df[\"price_to_book\"] = np.nan\n",
    "        else:\n",
    "            data_df.columns = [\"price_to_book\"]\n",
    "            data_df.index = data_df.index.str[0:10]#pd.to_datetime(data_df.index, format=\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "            ticker_info_df = ticker_info_df.join(data_df)\n",
    "            ticker_info_df[\"price_to_book\"] = ticker_info_df[\"price_to_book\"].fillna(method=\"ffill\")\n",
    "\n",
    "        # Price-to-free-cashflow\n",
    "        date_field = f\"TR.HistPricetoFreeOperatingCashFlowperShareAvgDilutedSharesOut(SDate={start},EDate={end},Period=FI0,Frq=FI).date\"\n",
    "        field = f\"TR.HistPricetoFreeOperatingCashFlowperShareAvgDilutedSharesOut(SDate={start},EDate={end},Period=FI0,Frq=FI)\"\n",
    "        eik_data, err = ek.get_data(ticker, [date_field, field])\n",
    "        data_df = pd.DataFrame(eik_data, columns=[\"Date\", \"Historic Price/FOCF/Shr (dil.)\"]).set_index(\"Date\")\n",
    "        if data_df[\"Historic Price/FOCF/Shr (dil.)\"].isna().all():\n",
    "            ticker_info_df[\"price_to_fcf\"] = np.nan\n",
    "        else:\n",
    "            data_df.columns = [\"price_to_fcf\"]\n",
    "            data_df.index = data_df.index.str[0:10]#pd.to_datetime(data_df.index, format=\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "            ticker_info_df = ticker_info_df.join(data_df)\n",
    "            ticker_info_df[\"price_to_fcf\"] = ticker_info_df[\"price_to_fcf\"].fillna(method=\"ffill\")\n",
    "\n",
    "        # EV-EBITDA\n",
    "        date_field = f\"TR.HistEnterpriseValueEBITDA(SDate={start},EDate={end},Period=FY0,Frq=FY).date\"\n",
    "        field = f\"TR.HistEnterpriseValueEBITDA(SDate={start},EDate={end},Period=FY0,Frq=FY)\"\n",
    "        eik_data, err = ek.get_data(ticker, [date_field, field])\n",
    "        data_df = pd.DataFrame(eik_data, columns=[\"Date\", \"Historic EV/EBITDA, FY\"]).set_index(\"Date\")\n",
    "        if data_df[\"Historic EV/EBITDA, FY\"].isna().all():\n",
    "            ticker_info_df[\"ev_ebitda\"] = np.nan\n",
    "        else:\n",
    "            data_df.columns = [\"ev_ebitda\"]\n",
    "            data_df.index = data_df.index.str[0:10]#pd.to_datetime(data_df.index, format=\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "            ticker_info_df = ticker_info_df.join(data_df)\n",
    "            ticker_info_df[\"ev_ebitda\"] = ticker_info_df[\"ev_ebitda\"].fillna(method=\"ffill\")\n",
    "\n",
    "#         # Current ratio\n",
    "#         date_field = f\"TR.CurrentRatio(SDate={start},EDate={end},Frq=Y).date\"\n",
    "#         field = f\"TR.CurrentRatio(SDate={start},EDate={end},Frq=Y)\"\n",
    "#         eik_data, err = ek.get_data(ticker, [date_field, field])\n",
    "#         data_df = pd.DataFrame(eik_data, columns=[\"Date\", \"Current Ratio\"]).set_index(\"Date\")\n",
    "#         data_df.columns = [\"current_ratio\"]\n",
    "#         data_df.index = data_df.index.str[0:10]#pd.to_datetime(data_df.index, format=\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "#         ticker_info_df = ticker_info_df.join(data_df)\n",
    "#         ticker_info_df[\"current_ratio\"] = ticker_info_df[\"current_ratio\"].fillna(method=\"ffill\")\n",
    "\n",
    "#         # Quick Ratio\n",
    "#         date_field = f\"TR.QuickRatio(SDate={start},EDate={end},Frq=Y).date\"\n",
    "#         field = f\"TR.QuickRatio(SDate={start},EDate={end},Frq=Y)\"\n",
    "#         eik_data, err = ek.get_data(ticker, [date_field, field])\n",
    "#         data_df = pd.DataFrame(eik_data, columns=[\"Date\", \"Quick Ratio\"]).set_index(\"Date\")\n",
    "#         data_df.columns = [\"quick_ratio\"]\n",
    "#         data_df.index = data_df.index.str[0:10]#pd.to_datetime(data_df.index, format=\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "#         ticker_info_df = ticker_info_df.join(data_df)\n",
    "#         ticker_info_df[\"quick_ratio\"] = ticker_info_df[\"quick_ratio\"].fillna(method=\"ffill\")\n",
    "\n",
    "#         # Dollar Turnover\n",
    "#         date_field = f\"TR.TURNOVER(SDate={start},EDate={end},Frq=D).date\"\n",
    "#         field = f\"TR.TURNOVER(SDate={start},EDate={end},Frq=D)\"\n",
    "#         eik_data, err = ek.get_data(ticker, [date_field, field])\n",
    "#         data_df = pd.DataFrame(eik_data, columns=[\"Date\", \"Turnover\"]).set_index(\"Date\")\n",
    "#         data_df.columns = [\"dollar_turnover\"]\n",
    "#         data_df.index = data_df.index.str[0:10]#pd.to_datetime(data_df.index, format=\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "#         ticker_info_df = ticker_info_df.join(data_df)\n",
    "        \n",
    "        # Because duplicate rows arise, we can't create SQL primary keys until we get rid of these duplicates...\n",
    "        ticker_info_df = ticker_info_df.loc[~ticker_info_df.index.duplicated(keep='first')]\n",
    "        \n",
    "        # Since '.' has a specific function in SQL, we use '_' instead.\n",
    "        db.append_df_as_row(ticker_info_df, ticker.replace(\".\",\"_\"), if_exists=\"replace\", index=True) \n",
    "        db.set_primary_key(ticker.replace(\".\",\"_\"), \"Date\")\n",
    "        \n",
    "        print(f\"-I- Successfully stored table for ({count}) {ticker}. {count-1} more to go.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        problem_stocks.append(ticker)\n",
    "        print(f\"-W- Could not store table for ({count}) {ticker}. Error message: {str(e)}\")\n",
    "\n",
    "\n",
    "print(\"\\n\",f\"Whole thing took {time.time() - time_} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(problem_stocks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used the cell below to remove stocks that could not be updated. I used the year as the criteria: if the latest row is from year 2019, then the table was not updated. Otherwise, it would be 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "string = \"SELECT TABLE_NAME FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_SCHEMA = \\\"SMFQuantDB\\\"\"\n",
    "stocks = pd.read_sql(string, db.engine).iloc[0:2466]\n",
    "response = []\n",
    "\n",
    "for count, stock in enumerate(stocks.values.flatten()):\n",
    "    string = \"SELECT MAX(DATE) FROM \" + stock\n",
    "    date = pd.to_datetime(pd.read_sql(string, db.engine).values[0][0])\n",
    "    if date.date().year == 2019:\n",
    "        db.delete_table(stock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_date = np.datetime64('2019-11-15')\n",
    "pd.to_datetime(test_date).date().year == 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing out volatility calculator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price = db.fetch_data_series(\"close_price\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volatilities = db.fetch_table(\"volatilities_as_at_2020_01_24\").set_index(\"ric\")\n",
    "volatilities.index = [ric.replace(\".\", \"_\") for ric in volatilities.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"... from the standard deviation of day to day logarithmic historical price changes... the annualized standard deviation of the relative price change for the 60 most recent trading days closing price, expressed as a percentage.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_ = time.time()\n",
    "tickers_list= [\"PTLA.OQ\"]\n",
    "start = dt.date(2016, 6, 30).strftime(\"%Y-%m-%d\")\n",
    "end = dt.datetime.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "problem_stocks = []\n",
    "\n",
    "for (count, ticker) in reversed(list(enumerate(tickers_list))):\n",
    "    try:\n",
    "        # Closing prices\n",
    "        ek_closing_prices = ek.get_timeseries(ticker, [\"Close\"], start_date=start, end_date=end)\n",
    "        ticker_info_df = pd.DataFrame(ek_closing_prices)\n",
    "        ticker_info_df.columns = [\"close_price\"]\n",
    "        ticker_info_df = ticker_info_df.loc[~ticker_info_df.index.duplicated(keep='first')]\n",
    "        print(f\"-I- Successfully stored table for ({count}) {ticker}. {count-1} more to go.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        problem_stocks.append(ticker)\n",
    "        print(f\"-W- Could not store table for ({count}) {ticker}. Error message: {str(e)}\")\n",
    "\n",
    "\n",
    "print(\"\\n\",f\"Whole thing took {time.time() - time_} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def volatility(period, as_at, price_table):\n",
    "    business_days_per_year = 224.5819722273368 # This number was obtained by comparing with the annualized historical\n",
    "                                               # volatility given by Eikon on the DIB for TR.Volatility60D, etc.\n",
    "    as_at = pd.Timestamp(as_at)\n",
    "    # Calculate log-returns\n",
    "    returns = price_table.dropna().pct_change() + 1\n",
    "    log_returns = returns.transform(np.log)\n",
    "    \n",
    "    # Calculate standard deviation of log-returns (as a percentage)\n",
    "    log_returns = pd.DataFrame(log_returns.loc[:as_at,:].iloc[-60:])#* np.sqrt(business_days_per_year))\n",
    "    volatilities = log_returns.std() * 100\n",
    "    \n",
    "    # Rename headings/column name\n",
    "    volatilities.index.name = \"ric\"\n",
    "    volatilities.columns = [f\"{period}_day_volatility\"]\n",
    "    \n",
    "    return(log_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "period = 60\n",
    "as_at = \"2020-01-24\"\n",
    "new = volatility(period, as_at, price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.DataFrame(index=new.index)\n",
    "test[\"new\"] = new\n",
    "test[\"old\"] = volatilities.loc[:,\"60_day_volatility\"]\n",
    "test[\"ratio\"] = (test[\"old\"] / test[\"new\"])**2\n",
    "test[\"ratio\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.loc[\"PTLA_OQ\",:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptla_price = db.fetch_data_series(\"close_price\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptla_price = ptla_price.loc[:,\"PTLA_OQ\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptla_price.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ptla_price.loc[:,ptla_price.isna().all()]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
